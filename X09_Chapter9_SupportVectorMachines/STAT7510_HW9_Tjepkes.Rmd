---
title: "STAT 7510 - Textbook HW9"
author: "Benjamin Tjepkes"
date: "2024-07-16"
output:
  html_document:
    toc: true
    toc_depth: 2
    df_print: paged
  word_document:
    toc: true
    reference_docx: "C:/Users/btjep/OneDrive/A_School/Mizzou/Coursework/STAT_7510
      - Applied Statistical Models I/X00_Logistics/custom_docx_template.docx"
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Instructions

From the course textbook, An Introduction to Statistical Learning with Applications in R Second Edition, **Chapter 9, Problems 1, 7, and 8**.

## Problem 1

### 1.A

**Sketch the hyperplane 1+3X1 − X2 =0. Indicate the set of points for which 1+3X1 − X2 > 0, as well as the set of points for which 1+3X1 − X2 < 0.**

```{r}

# Part A
x1 = -5:5
x2 = 1 + 3 * x1
plot(x1, x2, type = "l", col = "purple", lwd = 3)
text(c(0), c(-12), "> 0", col = "purple")
text(c(0), c(12), "< 0", col = "purple")

# Part B
lines(x1, 1 - x1/2, lwd = 3)
text(c(0), c(-8), "< 0")
text(c(0), c(8), "> 0")
```


### 1.B

**On the same plot, sketch the hyperplane −2+X1 +2X2 =0. Indicate the set of points for which −2+X1 +2X2 > 0, as well as the set of points for which −2+X1 +2X2 < 0.**

See above plot, line in black.

## Problem 7

### 7.A

**Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.**

```{r}
# Load in the dataset
Auto <- ISLR2::Auto

# Create mileage class based on median value split, 1 = high | 0 = low
Auto$mpg_class <- as.factor(ifelse(Auto$mpg > median(Auto$mpg), yes = 1, no = 0))
```


### 7.B

**Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results. Note you will need to fit the classifier without the gas mileage variable to produce sensible results.**

```{r}
# Load in library for SVM functions
library(e1071)

# Subset to remove original mpg variable
Auto_trimmed <- Auto[ , -which(names(Auto) %in% c("mpg"))]

# Compare a range of tuning parameters
set.seed(1)
cars_cv <- e1071::tune(svm, mpg_class ~ ., data = Auto_trimmed, kernel = "linear",
                       ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
summary(cars_cv)
plot(cars_cv)
```

It looks like CV error is minimized when $cost = 0.1$, then steeply increases after that for tested values 1, 5, 10, and 100.


### 7.C

**Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results.**

```{r}
# Compare a range of tuning parameters
set.seed(1)
cars_cv <- e1071::tune(svm, mpg_class ~ ., data = Auto_trimmed, kernel = "radial",
                       ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100),
                                     gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(cars_cv)
```

With the radial kernel, the lowest CV error was achieved with $cost = 10$ and $gamma = 1$.


```{r}
# Compare a range of tuning parameters
set.seed(1)
cars_cv <- e1071::tune(svm, mpg_class ~ ., data = Auto_trimmed, kernel = "polynomial",
                       ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100),
                                     degree = c(2, 3, 4)))
summary(cars_cv)
```

With the polynomial kernel, the lowest CV error was achieved with $cost = 100$ and $degree = 2$.


### 7.D

**Make some plots to back up your assertions in (b) and (c).**

```{r}
svm.linear  <-  svm(mpg_class ~ ., data = Auto, kernel = "linear", cost = 1)
svm.radial <-  svm(mpg_class ~ ., data = Auto, kernel = "radial", cost = 10, gamma = 0.01)
svm.poly <-  svm(mpg_class ~ ., data = Auto, kernel = "polynomial", cost = 10, degree = 2)

models <- list(svm.linear, svm.radial, svm.poly)

for (model in models) {

    for (name in names(Auto)[!(names(Auto) %in% c("mpg", "mpg_class", "name"))]) {
        plot(model, Auto, as.formula(paste("mpg~", name, sep = "")))
    }

}
```


## Problem 8

### 8.A

**Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.**

```{r}
# Load in dataset
OJ <- ISLR2::OJ

# Create an index for training data
set.seed(1)
train_index <- sample(1:nrow(OJ), 800) # 800 samples

# Split dataset
OJ_train <- OJ[train_index, ]
OJ_test <- OJ[-train_index, ]
```


### 8.B

**Fit a support vector classifier to the training data using cost = 0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.**

```{r}
# Fit linear SVM on training data
OJ.svm.fit <- e1071::svm(Purchase ~ ., data = OJ_train, kernel = "linear",
                     cost = 0.01, scale = TRUE)

# Print summary of model fit
summary(OJ.svm.fit)
```


The resulting model, with $cost = 0.01$, has 432 total support vectors. This includes 219 for CH and 216 for MM.


### 8.C

**What are the training and test error rates?**


```{r}
# Predict response values using the fitted model and training set
OJ.train.predict <- predict(OJ.svm.fit, data = OJ_train)

# Print out a table of classifications for training set
table(OJ.train.predict, OJ_train$Purchase)

# Calculate train MSE
(trainMSE <- mean(OJ.train.predict != OJ_train$Purchase))
```

```{r}
# Predict response values using the fitted model and test set
OJ.test.predict <- predict(OJ.svm.fit, newdata = OJ_test)

# Print out a table of classifications for test set
table(OJ.test.predict, OJ_test$Purchase)

# Calculate test MSE
(testMSE <- mean(OJ.test.predict != OJ_test$Purchase))
```

The train MSE is `r trainMSE` and the test MSE is `r testMSE`.


### 8.D

**Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.**

```{r}
# Use the tune function to find optimal cost parameter
set.seed(1)
OJ.tune <- e1071::tune(METHOD = svm, Purchase ~ ., data = OJ_train, kernel = "linear",
                       ranges = list(cost = c(0.01, 0.1, 1, 2.5, 5, 7.5, 10)))
# Print tuning summary
summary(OJ.tune)
```

The optimal cost parameter is 2.5.

### 8.E

**Compute the training and test error rates using this new value for cost.**

```{r}
# Assign best model to model variable
OJ.tune.fit <- OJ.tune$best.model

# Get train MSE
OJ.tune.train <- predict(OJ.tune.fit, data = OJ_train)
(trainMSE <- mean(OJ.tune.train != OJ_train$Purchase))

# Get test MSE
OJ.tune.test <- predict(OJ.tune.fit, newdata = OJ_test)
(testMSE <- mean(OJ.tune.test != OJ_test$Purchase))

```

The train MSE is `r trainMSE` and the test MSE is `r testMSE`.


### 8.F

**Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.**

```{r}
# Fit the SVM with radial
OJ.svm.fit <- e1071::svm(Purchase ~ ., data = OJ_train, kernel = "radial",
                         cost = 0.01, scale = T)
# Print summary
summary(OJ.svm.fit)
```


```{r}
# Predict response values using the fitted model and training set
OJ.train.predict <- predict(OJ.svm.fit, data = OJ_train)

# Print out a table of classifications for training set
table(OJ.train.predict, OJ_train$Purchase)

# Calculate train MSE
(trainMSE <- mean(OJ.train.predict != OJ_train$Purchase))

```


```{r}
# Predict response values using the fitted model and test set
OJ.test.predict <- predict(OJ.svm.fit, newdata = OJ_test)

# Print out a table of classifications for test set
table(OJ.test.predict, OJ_test$Purchase)

# Calculate test MSE
(testMSE <- mean(OJ.test.predict != OJ_test$Purchase))
```


```{r}
# Use the tune function to find optimal cost parameter
set.seed(1)
OJ.tune <- e1071::tune(METHOD = svm, Purchase ~ ., data = OJ_train, kernel = "radial",
                       ranges = list(cost = c(0.01, 0.1, 1, 2.5, 5, 7.5, 10)))
# Print tuning summary
summary(OJ.tune)
```
```{r}
# Assign best model to model variable
OJ.tune.fit <- OJ.tune$best.model

# Get train MSE
OJ.tune.train <- predict(OJ.tune.fit, data = OJ_train)
(trainMSE <- mean(OJ.tune.train != OJ_train$Purchase))

# Get test MSE
OJ.tune.test <- predict(OJ.tune.fit, newdata = OJ_test)
(testMSE <- mean(OJ.tune.test != OJ_test$Purchase))
```


### 8.G

**Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2.**

```{r}
# Fit the SVM with radial
OJ.svm.fit <- e1071::svm(Purchase ~ ., data = OJ_train, kernel = "polynomial",
                         cost = 0.01, scale = T, degree = 2)
# Print summary
summary(OJ.svm.fit)
```


```{r}
# Predict response values using the fitted model and training set
OJ.train.predict <- predict(OJ.svm.fit, data = OJ_train)

# Print out a table of classifications for training set
table(OJ.train.predict, OJ_train$Purchase)

# Calculate train MSE
(trainMSE <- mean(OJ.train.predict != OJ_train$Purchase))

```


```{r}
# Predict response values using the fitted model and test set
OJ.test.predict <- predict(OJ.svm.fit, newdata = OJ_test)

# Print out a table of classifications for test set
table(OJ.test.predict, OJ_test$Purchase)

# Calculate test MSE
(testMSE <- mean(OJ.test.predict != OJ_test$Purchase))
```


```{r}
# Use the tune function to find optimal cost parameter
set.seed(1)
OJ.tune <- e1071::tune(METHOD = svm, Purchase ~ ., data = OJ_train, kernel = "radial",
                       ranges = list(cost = c(0.01, 0.1, 1, 2.5, 5, 7.5, 10)))
# Print tuning summary
summary(OJ.tune)
```
```{r}
# Assign best model to model variable
OJ.tune.fit <- OJ.tune$best.model

# Get train MSE
OJ.tune.train <- predict(OJ.tune.fit, data = OJ_train)
(trainMSE <- mean(OJ.tune.train != OJ_train$Purchase))

# Get test MSE
OJ.tune.test <- predict(OJ.tune.fit, newdata = OJ_test)
(testMSE <- mean(OJ.tune.test != OJ_test$Purchase))
```

### 8.H

**Overall, which approach seems to give the best results on this data?**

After tuning, the linear kernel produced the lowest training and test errors on these data. Before tuning, the radial and polynomial kernel models performed very poorly, but these errors were greatly reduced after finding optimal values for the cost parameter.


## Session Info

```{r}
sessionInfo()
```

