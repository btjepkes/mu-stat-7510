---
title: "STAT 7510 - Textbook HW12"
author: "Benjamin Tjepkes"
date: "2024-07-19"
output:
  html_document:
    toc: true
    toc_depth: 2
    df_print: paged
  word_document:
    toc: true
    reference_docx: "C:/Users/btjep/OneDrive/A_School/Mizzou/Coursework/STAT_7510
      - Applied Statistical Models I/X00_Logistics/custom_docx_template.docx"
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Instructions

From the course textbook, An Introduction to Statistical Learning with Applications in R Second Edition, **Chapter 12, Problems 9, 10, and 13**.


## Problem 9

### 9.A

**Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.**

```{r, fig.width = 9, fig.height = 8}
# Perform hierarchical clustering with the complete linkage method
hc.arrests <- hclust(dist(USArrests), method = "complete")

# Plot the tree
plot(hc.arrests, main="Complete Linkage w/o Scaling", xlab = "", sub = "", cex=.9)
```


### 9.B

**Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?**

```{r}
# Cut dendrogram at k=3
(hc.cut3 <- cutree(tree = hc.arrests, k = 3))

# View the classes for each state
table(hc.cut3)
```


### 9.C

**Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.**

```{r, fig.width = 9, fig.height = 8}
# Scale variables
USArrests_scaled <- scale(USArrests)

# Perform hierarchical clustering with the complete linkage method
hc.arrests_scaled <- hclust(dist(USArrests_scaled), method = "complete")

# Plot the tree
plot(hc.arrests_scaled, main="Complete Linkage with Scaling", xlab = "", sub = "", cex=.9)
```


### 9.D

**What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.**

```{r}
# Cut dendrogram at k=3
(hc.cut3_scaled <- cutree(tree = hc.arrests_scaled, k = 3))
```

```{r}
# Examine the number of obs in each cluster
table(hc.cut3_scaled)
```

Scaling the variables changed the resulting cluster segmentation, especially for cluster 1 when maintaining the $k=3$ setting. Yes, I think these variables should be scaled because of the $UrbanPop$ variable (a proportion) is measured differently than the other variables (which are rates).


## Problem 10

### 10.A

**Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.**

```{r}
# Set seed for reproducibility
set.seed(1)

# Create a simulated dataset with varying means to form three clusters of points
sim_data <- rbind(matrix(rnorm(20*50, mean = 0), nrow = 20),
            matrix(rnorm(20*50, mean = 1), nrow = 20),
            matrix(rnorm(20*50, mean = 2), nrow = 20))
```


### 10.B

**Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.**

```{r}
# Perform PCA with scaled data
pca.sim <- prcomp(sim_data, scale = TRUE)

# Plot the output of PCA
plot(pca.sim$x[ ,1:2], , col=c(rep(1,20), rep(2,20), rep(3,20)))
```


### 10.C

**Perform K-means clustering of the observations with K =3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?**

```{r}
# Set seed for reproducibility
set.seed(1)

# Run k-means with k=3
km.sim <- kmeans(sim_data, centers = 3)

# Create variable for true classes for cluster points
true_class = c(rep(1,20), rep(2,20), rep(3,20))

# Compare classifications
table(km.sim$cluster, true_class)
```


### 10.D

**Perform K-means clustering with K =2. Describe your results.**

```{r}
# Set seed for reproducibility
set.seed(1)

# Run k-means with k=2
km.sim <- kmeans(sim_data, centers = 2)

# Compare classifications
table(km.sim$cluster, true_class)
```

The number of mis-classifications increases greatly likely because the intermediate cluster is split between some of the outer two clusters.


### 10.E

**Now perform K-means clustering with K =4, and describe your results.**

```{r}
# Set seed for reproducibility
set.seed(1)

# Run k-means with k=4
km.sim <- kmeans(sim_data, centers = 4)

# Compare classifications
table(km.sim$cluster, true_class)
```

It looks like the intermediate cluster was divided into the new cluster.


### 10.F

**Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60 Ã— 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.**

```{r}
# Set seed for reproducibility
set.seed(1)

# Run K-means with k=3 clusters
km.pca <- kmeans(pca.sim$x[,1:2], centers = 3)

# Compare classifications
table(km.pca$cluster, true_class)
```

Similar to the K-means on the raw data, with very low misclassification rate.

### 10.G

**Using the `scale()` function, perform K-means clustering with K =3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.**

```{r}
# Set seed for reproducibility
set.seed(1)

# Run k-means with k=3
km.sim <- kmeans(scale(sim_data), centers = 3)

# Compare classifications
table(km.sim$cluster, true_class)
```

It looks like the scaled variables resulted in the same classification, with a very low misclassification rate.



## Problem 13

### 13.A

**Load in the data using `read.csv()`. You will need to select `header = F`.**

```{r}
gene_exp <- read.csv(file = "../X00_Data/Ch12Ex13.csv", header = F)
```


### 13.B

**Apply hierarchical clustering to the samples using correlation based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?**

```{r}
# Perform hierarchical clustering, with complete linkage method
hc.gene <- hclust(as.dist(1 - cor(gene_exp)), method = "complete")

# Plot the dendrogram
plot(hc.gene)
```

```{r}
# Perform hierarchical clustering, with single linkage method
hc.gene <- hclust(as.dist(1 - cor(gene_exp)), method = "single")

# Plot the dendrogram
plot(hc.gene)
```

```{r}
# Perform hierarchical clustering, with average linkage method
hc.gene <- hclust(as.dist(1 - cor(gene_exp)), method = "average")

# Plot the dendrogram
plot(hc.gene)
```


The genes divide the samples into two groups only when the linkage method of single is used, else the dendrograms show 3 or 4 distinct groupings.


### 13.C

**Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.**

By running a PCA on the gene expression data, you can examine the variable loadings to see the genes that greatly contribute to the groups. Here I have examined just the PC1 loadings.

```{r}
# Transform the matrix so genes are features
gene_trans <- t(gene_exp)

# Rename rows
rownames(gene_trans) <- 1:40

# Perform PCA on gene expression data
pca.gene <- prcomp(gene_trans, scale = TRUE)

# Pull out loadings for PC1
pca.gene.loadings <- pca.gene$rotation[,1]

# Print the loadings of the first component
sort(abs(pca.gene.loadings[1:20]),decreasing = T)
```


## Session Info

```{r}
sessionInfo()
```

